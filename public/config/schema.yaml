- name: "GPT1"
  data:
    dataset: "Unreleased"
    tokenizer: "GPT-1"
  architecture:
    embeddings: "Learned"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLu"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "Not Disclosed"
  optimization:
    lr_warmup: ""
    lr_decay: "Cosine to 0"
    precision: "Not Disclosed"
    clipping: "Not Disclosed"
    dropout: 0.1
    weight_decay: Not disclosed
  misc:
    training_library: "Unreleased"
    date: 6/11/2018
    source: "https://cdn.openai.com/research-covers/language-   unsupervised/language_understanding_paper.pdf"
- name: "GPT-2"
  details:
    dataset: "Unreleased"
    tokenizer: "GPT-2"
    training_library: "Unreleased"
    embeddings: "Learned"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "Not Disclosed"
    lr_warmup: ""
    lr_decay: "Cosine to 0"
    precision: "Not Disclosed"
    clipping: "Not Disclosed"
    dropout: 0.1
    weight_decay: "Not disclosed"
    date: "February 14, 2019"
    source: ""
- name: "GPT-3"
  details:
    dataset: "Unreleased"
    tokenizer: "GPT-2"
    training_library: "Unreleased"
    embeddings: "Learned"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / fp16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "Alternating sparse and dense layers"
    date: "May 28, 2020"
    source: "https://arxiv.org/abs/2005.14165"
- name: "GPT-Neo"
  details:
    dataset: "the Pile"
    tokenizer: "GPT-2"
    training_library: "GPT-Neo"
    embeddings: "Learned"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to ???"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "Sliding window attention"
    date: "March 22, 2021"
    source: "https://github.com/EleutherAI/gpt-neo/blob/master/configs/gpt3_XL_256_Pile.json"
- name: "GPT-J"
  details:
    dataset: "the Pile"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "the Pile"
    normalization: "Unreleased"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.999, 1e-08"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "Cosine to 10%"
    date: "June 8, 2021"
    source: "Config file"

- name: "FairSeq Dense"
  details:
    dataset: "Unreleased"
    tokenizer: "GPT-2"
    training_library: "GPT-2"
    embeddings: "Unreleased"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.98"
    lr_warmup: "Linear to 0"
    lr_decay: "Pure fp16"
    precision: "1.0"
    clipping: "0.1"
    dropout: "0.01"
    weight_decay: "Not disclosed"
    date: "December 20, 2021"
    source: "Paper"
- name: "Gopher"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "GPT-NeoX-20B"
    normalization: "Unreleased"
    parallel_layers: "No"
    biases: "No"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / fp16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "Not disclosed"
    date: "December 8, 2021"
    source: "Paper"
- name: "GPT-NeoX"
  details:
    dataset: "the Pile"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "the Pile + Unreleased"
    normalization: "ROOTS"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / fp16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.0"
    weight_decay: "0.01"
    date: "February 2, 2022"
    source: "Paper"

- name: "PaLM"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "Unreleased"
    parallel_layers: "Yes"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: 4
    optimizer: "Adafactor w/o factorization"
    optimizer_hyperparams: "0.9, 1 - step_num^-0.8"
    lr_warmup: "1/sqrt(step_num)"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.1"
    dropout: "0.0"
    weight_decay: "lr^2.0"
    date: "April 4, 2022"
    source: "Paper"

- name: "OPT"
  details:
    dataset: "the Pile + Unreleased"
    tokenizer: "GPT-2"
    training_library: "GPT-2"
    embeddings: "BLOOM"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "ReLU"
    d_attn_d_ff: 4
    optimizer: "AdamW"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Custom to 10%"
    lr_decay: "fp32 / fp16"
    precision: "1.0"
    clipping: "0.1"
    dropout: "0.10"
    weight_decay: "0.10"
    date: "May 2, 2022"
    source: "Paper"

- name: "BLOOM"
  details:
    dataset: "ROOTS"
    tokenizer: "GPT-NeoX-20B"
    training_library: "Unreleased"
    embeddings: "GPT-2"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to ???"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "0.10"
    date: "May 26, 2022"
    source: "Paper"

- name: "Pythia"
  details:
    dataset: "the Pile"
    tokenizer: "GPT-NeoX-20B"
    training_library: "GPT-NeoX"
    embeddings: "Rotary"
    normalization: "LayerNorm"
    parallel_layers: "Yes"
    biases: "Yes"
    activation_function: "GeLu"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: ""
    lr_decay: "Cosine to 10%"
    precision: "fp32 / fp16"
    clipping: "1.0"
    dropout: 0.0
    weight_decay: 0.1
    date: 12/10/2022
    source: "https://arxiv.org/abs/2304.01373"

- name: "LLaMA"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "RMSNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "AdamW"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Not disclosed"
    lr_decay: "Cosine to 10%"
    precision: "fp32 / fp16"
    clipping: "1.0"
    dropout: "0.0"
    weight_decay: "0.10"
    date: "February 24, 2023"
    source: "Paper"

- name: "RedPajama-INCITE"
  details:
    dataset: "RedPajamas"
    tokenizer: "GPT-NeoX-20B"
    training_library: "GPT-NeoX-20B"
    embeddings: "GPT-NeoX-20B"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Not disclosed"
    optimizer_hyperparams: ""
    lr_warmup: "Not disclosed"
    lr_decay: "Not disclosed"
    precision: "Not disclosed"
    clipping: "Not disclosed"
    dropout: "Not disclosed"
    weight_decay: "Not disclosed"
    date: "May 5, 2023"
    source: "Paper"

- name: "MPT"
  details:
    dataset: "C4, RP, Stack, S2ORC"
    tokenizer: "Composer"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "QKNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "LION"
    optimizer_hyperparams: "0.9, 0.95, 1e-9"
    lr_warmup: "Linear over 375M tokens"
    lr_decay: "Cosine to 10%"
    precision: "fp32 / bf16"
    clipping: "1.0"
    dropout: "0.0"
    weight_decay: "0.0"
    date: "May 5, 2023"
    source: "Paper"

- name: "CerebrasGPT"
  details:
    dataset: "the Pile"
    tokenizer: "Unreleased"
    training_library: "GPT-2"
    embeddings: "Unreleased"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.10"
    weight_decay: "Not disclosed"
    date: "Not disclosed"
    source: "Uses muP (but only up to 3B!)"

- name: "LLaMA 2"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "the Pile + Unreleased"
    embeddings: "Unreleased"
    normalization: "Unreleased"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "AdamW"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.1"
    weight_decay: "0.10"
    date: "July 18, 2023"
    source: "Paper"

- name: "WeLab"
  details:
    dataset: "Unreleased"
    tokenizer: "GPT-NeoX"
    training_library: "GPT-NeoX"
    embeddings: "GPT-NeoX"
    normalization: "LayerNorm"
    parallel_layers: "Yes"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "Not disclosed"
    lr_warmup: "Not disclosed"
    lr_decay: "Cosine to 10%"
    precision: "Not disclosed"
    clipping: "Not disclosed"
    dropout: "0.0"
    weight_decay: "0.0001"
    date: "August 5, 2023"
    source: "Config file"

- name: "Stable LM v2"
  details:
    dataset: "the Pile + Unreleased"
    tokenizer: "GPT-NeoX"
    training_library: "GPT-NeoX"
    embeddings: "Unreleased"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "AdamW"
    optimizer_hyperparams: "0.9, 0.95, 1e-8"
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.1"
    weight_decay: "Not disclosed"
    date: "September 6, 2023"
    source: "Model card"

- name: "Falcon-180B"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLU"
    d_attn_d_ff: 4
    optimizer: "AdamW"
    optimizer_hyperparams: "Not disclosed"
    lr_warmup: "Not disclosed"
    lr_decay: "Cosine to 10%"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.0"
    weight_decay: "Not disclosed"
    date: "September 27, 2023"
    source: "Paper"

- name: "Mistral"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "RMSNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "Not disclosed"
    optimizer_hyperparams: ""
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.1"
    dropout: "0.10"
    weight_decay: "Sliding window attention"
    date: "September 24, 2023"
    source: "Paper"

- name: "Qwen"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "RMSNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "Not disclosed"
    optimizer_hyperparams: ""
    lr_warmup: "Not disclosed"
    lr_decay: "Not disclosed"
    precision: "Not disclosed"
    clipping: "Not disclosed"
    dropout: "0.0"
    weight_decay: "Not disclosed"
    date: "Not disclosed"
    source: "Paper"

- name: "Yi"
  details:
    dataset: "Unreleased"
    tokenizer: "Unreleased"
    training_library: "Unreleased"
    embeddings: "Unreleased"
    normalization: "RMSNorm"
    parallel_layers: "No"
    biases: "No"
    activation_function: "SwiGLU"
    d_attn_d_ff: "8/3"
    optimizer: "Not disclosed"
    optimizer_hyperparams: ""
    lr_warmup: "Cosine to 10%"
    lr_decay: "fp32 / bf16"
    precision: "1.0"
    clipping: "0.0"
    dropout: "0.1"
    weight_decay: "Uses muP (but only up to 3B!)"
    date: "Not disclosed"
    source: "Paper"
