- name: "GPT1"
  details:
    dataset: "Unreleased"
    tokenizer: "GPT-1"
    training_library: "Unreleased"
    embeddings: "Learned"
    normalization: "LayerNorm"
    parallel_layers: "No"
    biases: "Yes"
    activation_function: "GeLu"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "Not Disclosed"
    lr_warmup: ""
    lr_decay: "Cosine to 0"
    precision: "Not Disclosed"
    clipping: "Not Disclosed"
    dropout: 0.1
    weight_decay: Not disclosed
    date: 6/11/2018
    source: "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
- name: "Pythia"
  details:
    dataset: "the Pile"
    tokenizer: "GPT-NeoX-20B"
    training_library: "GPT-NeoX"
    embeddings: "Rotary"
    normalization: "LayerNorm"
    parallel_layers: "Yes"
    biases: "Yes"
    activation_function: "GeLu"
    d_attn_d_ff: 4
    optimizer: "Adam"
    optimizer_hyperparams: "0.9, 0.95"
    lr_warmup: ""
    lr_decay: "Cosine to 10%"
    precision: "fp32 / fp16"
    clipping: "1.0"
    dropout: 0.0
    weight_decay: 0.1
    date: 12/10/2022
    source: "https://arxiv.org/abs/2304.01373"

